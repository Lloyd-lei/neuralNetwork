<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>MNIST Model Comparison: MLP, CNN and Transformer</title>
    <style>
      body {
        font-family: "Arial", sans-serif;
        line-height: 1.6;
        color: #333;
        max-width: 1200px;
        margin: 0 auto;
        padding: 20px;
        background-color: #f5f5f5;
      }
      h1,
      h2,
      h3 {
        color: #2c3e50;
      }
      h1 {
        text-align: center;
        border-bottom: 2px solid #3498db;
        padding-bottom: 10px;
        margin-bottom: 30px;
      }
      .container {
        background-color: white;
        padding: 25px;
        border-radius: 8px;
        box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        margin-bottom: 30px;
      }
      .image-container {
        text-align: center;
        margin: 20px 0;
      }
      img {
        max-width: 100%;
        height: auto;
        border-radius: 5px;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
      }
      .explanation {
        background-color: #f8f9fa;
        padding: 15px;
        border-left: 4px solid #3498db;
        margin: 20px 0;
      }
      .model-comparison {
        display: flex;
        justify-content: space-between;
        margin: 20px 0;
        flex-wrap: wrap;
      }
      .model-card {
        flex: 1;
        margin: 10px;
        padding: 15px;
        background-color: #f8f9fa;
        border-radius: 5px;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        min-width: 250px;
      }
      .mlp {
        border-left: 4px solid #ff9671;
      }
      .cnn {
        border-left: 4px solid #00d2fc;
      }
      .transformer {
        border-left: 4px solid #9b59b6;
      }
      table {
        width: 100%;
        border-collapse: collapse;
        margin: 20px 0;
      }
      th,
      td {
        padding: 12px;
        text-align: left;
        border-bottom: 1px solid #ddd;
      }
      th {
        background-color: #f2f2f2;
      }
      tr:hover {
        background-color: #f5f5f5;
      }
      footer {
        text-align: center;
        margin-top: 30px;
        padding-top: 20px;
        border-top: 1px solid #ddd;
        color: #7f8c8d;
        font-size: 0.9em;
      }
      .back-btn {
        display: inline-block;
        padding: 10px 20px;
        background: #3498db;
        color: white;
        text-decoration: none;
        border-radius: 5px;
        margin-bottom: 20px;
      }
      .back-btn:hover {
        background: #2980b9;
      }
    </style>
  </head>
  <body>
    <a href="../index.html" class="back-btn">Back to Home</a>

    <h1>MNIST Model Comparison: MLP, CNN and Transformer</h1>

    <div class="container">
      <h2>Model Architecture Comparison</h2>

      <div class="explanation">
        <p>
          This project compares three different neural network architectures for
          MNIST handwritten digit recognition: Multi-Layer Perceptron (MLP),
          Convolutional Neural Network (CNN), and Transformer. The following
          shows the architectural differences between the three models.
        </p>
      </div>

      <div class="image-container">
        <img
          src="comparison_images/architecture_comparison_all.png"
          alt="Model Architecture Comparison"
        />
      </div>

      <div class="model-comparison">
        <div class="model-card mlp">
          <h3>MLP Model Features</h3>
          <ul>
            <li>Fully connected neural network structure</li>
            <li>Input layer: 784 neurons (28x28 pixels flattened)</li>
            <li>Hidden layers: 512 and 256 neurons</li>
            <li>Output layer: 10 neurons (corresponding to digits 0-9)</li>
            <li>Advantages: Fast training, simple structure</li>
            <li>
              Disadvantages: No spatial information preservation, large
              parameter count
            </li>
          </ul>
        </div>

        <div class="model-card cnn">
          <h3>CNN Model Features</h3>
          <ul>
            <li>Convolutional neural network structure</li>
            <li>Convolutional layers: Extract spatial features</li>
            <li>
              Pooling layers: Dimensionality reduction while preserving
              important features
            </li>
            <li>Fully connected layers: Final classification</li>
            <li>
              Advantages: Preserves spatial information, small parameter count,
              high accuracy
            </li>
            <li>Disadvantages: Longer training time, complex structure</li>
          </ul>
        </div>

        <div class="model-card transformer">
          <h3>Transformer Model Features</h3>
          <ul>
            <li>Attention-based structure</li>
            <li>Input embedding: Convert input to vector representation</li>
            <li>Positional encoding: Add positional information</li>
            <li>
              Multi-head attention: Capture relationships between different
              features
            </li>
            <li>
              Advantages: Strong parallel computing capability, can capture
              long-range dependencies
            </li>
            <li>Disadvantages: Large parameter count, long training time</li>
          </ul>
        </div>
      </div>
    </div>

    <div class="container">
      <h2>Performance Comparison</h2>

      <div class="explanation">
        <p>
          The following shows the performance comparison of the three models on
          the MNIST dataset, including test accuracy, training time, and model
          size metrics.
        </p>
      </div>

      <div class="image-container">
        <img
          src="comparison_images/radar_chart_all.png"
          alt="Performance Radar Chart"
        />
      </div>

      <div class="image-container">
        <img
          src="comparison_images/bar_comparison_all.png"
          alt="Performance Bar Comparison"
        />
      </div>

      <h3>Performance Metrics Comparison Table</h3>
      <table>
        <tr>
          <th>Metric</th>
          <th>MLP</th>
          <th>CNN</th>
          <th>Transformer</th>
        </tr>
        <tr>
          <td>Test Accuracy (%)</td>
          <td>98.10</td>
          <td>98.83</td>
          <td>96.80</td>
        </tr>
        <tr>
          <td>Training Time (seconds)</td>
          <td>35.37</td>
          <td>83.70</td>
          <td>164.68</td>
        </tr>
        <tr>
          <td>Model Size (MB)</td>
          <td>2.00</td>
          <td>0.18</td>
          <td>4.35</td>
        </tr>
      </table>

      <div class="explanation">
        <p>From the performance comparison, we can see:</p>
        <ul>
          <li>
            CNN model achieves the best accuracy (98.83%), while Transformer has
            relatively lower accuracy (96.80%) on this simple task
          </li>
          <li>
            MLP model has the fastest training speed, Transformer has the
            longest training time
          </li>
          <li>
            CNN model has the smallest size, Transformer model has the largest
            size
          </li>
          <li>
            For simple tasks like MNIST, traditional architectures (CNN/MLP) are
            more effective than Transformers
          </li>
        </ul>
      </div>
    </div>

    <div class="container">
      <h2>Training Process Comparison</h2>

      <div class="image-container">
        <img
          src="comparison_images/training_curves_all.png"
          alt="Training Curves Comparison"
        />
      </div>

      <div class="explanation">
        <p>The training curves show:</p>
        <ul>
          <li>
            MLP model converges fastest but has slightly lower final validation
            accuracy
          </li>
          <li>
            CNN model converges quickly in the middle stages with high final
            validation accuracy
          </li>
          <li>
            Transformer model converges slowly but achieves good final
            validation accuracy
          </li>
          <li>All models tend to stabilize in the later stages of training</li>
        </ul>
      </div>
    </div>

    <div class="container">
      <h2>Inference Results Comparison</h2>

      <div class="image-container">
        <img
          src="inference_images/combined_predictions_all.png"
          alt="Model Inference Results Comparison"
        />
      </div>

      <div class="explanation">
        <p>The inference results show:</p>
        <ul>
          <li>
            All three models can correctly identify handwritten digits in most
            cases
          </li>
          <li>
            CNN and Transformer perform better on some complex or blurry digits
          </li>
          <li>
            All three models may make errors on the same difficult samples
          </li>
          <li>Transformer model performs best in some edge cases</li>
        </ul>
      </div>

      <h3>Transformer Attention Visualization</h3>
      <div class="image-container">
        <img
          src="inference_images/transformer_attention.png"
          alt="Transformer Attention Visualization"
        />
      </div>

      <div class="explanation">
        <p>
          The Transformer model's attention mechanism visualization shows how
          the model focuses on different parts of the input image. The heatmap
          displays the regions the model pays attention to when making
          decisions, helping us understand the model's working principles.
        </p>
      </div>
    </div>

    <div class="container">
      <h2>Conclusion</h2>

      <p>
        Through comparing MLP, CNN, and Transformer models on the MNIST dataset,
        we can draw the following conclusions:
      </p>

      <ol>
        <li>
          All three models can effectively recognize handwritten digits with
          accuracy over 96%
        </li>
        <li>
          CNN model achieves the best balance between accuracy and efficiency,
          making it the optimal choice for this task
        </li>
        <li>
          MLP model, despite its simple structure, can achieve satisfactory
          recognition results in most cases with the fastest training speed
        </li>
        <li>
          Transformer model demonstrates the complexity of modern architectures
          but may be overkill for simple tasks like MNIST
        </li>
        <li>
          Model selection should be based on specific application scenarios,
          available computational resources, and accuracy requirements
        </li>
      </ol>

      <p>
        This comparison demonstrates the evolution of deep learning model
        architectures, from simple fully connected networks (MLP), to
        image-specialized convolutional networks (CNN), to recently
        widely-applied attention-based models (Transformer). Each architecture
        has its advantages and disadvantages, suitable for different application
        scenarios.
      </p>
    </div>

    <footer>
      <p>
        MNIST Model Comparison: MLP, CNN and Transformer | Physics 129 Course
        Project
      </p>
    </footer>
  </body>
</html>
